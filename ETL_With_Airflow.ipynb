{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOscfHr73HIb"
      },
      "outputs": [],
      "source": [
        "from datetime import timedelta, datetime\n",
        "\n",
        "\n",
        "from airflow import DAG \n",
        "from airflow.utils.dates import days_ago\n",
        "from airflow.operators.dummy_operator import DummyOperator\n",
        "from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator\n",
        "from airflow.providers.google.cloud.operators.bigquery import BigQueryCheckOperator\n",
        "from airflow.contrib.operators.bigquery_operator import BigQueryOperator\n",
        "\n",
        "\n",
        "\n",
        "GOOGLE_CONN_ID = \"google_cloud_default\"\n",
        "PROJECT_ID=\"db-systems-group-project\"\n",
        "GS_PATH = \"trips/\"\n",
        "BUCKET_NAME = 'master-data-bucket-group-5'\n",
        "STAGING_DATASET = \"trips_staging_dataset\"\n",
        "DATASET = \"recipe_dataset\"\n",
        "LOCATION = \"us (multiple regions in United States)\"\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'Koushik Modekurti',\n",
        "    'depends_on_past': False,\n",
        "    'email_on_failure': ['koushik.modekurti@sjsu.edu'],\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'start_date':  days_ago(2),\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "with DAG('ETL_With_Airflow', schedule_interval=timedelta(days=1), default_args=default_args) as dag:\n",
        "    start_pipeline = DummyOperator(\n",
        "        task_id = 'start_pipeline',\n",
        "        dag = dag\n",
        "        )\n",
        "\n",
        "\n",
        "    load_staging_dataset = DummyOperator(\n",
        "        task_id = 'load_staging_dataset',\n",
        "        dag = dag\n",
        "        )    \n",
        "    \n",
        "    load_dataset_master_trip = GCSToBigQueryOperator(\n",
        "        task_id = 'load_dataset_master_trip',\n",
        "        bucket = BUCKET_NAME,\n",
        "        source_objects = ['trips/Master_Trip_Data.csv'],\n",
        "        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.dataset_master_trip',\n",
        "        write_disposition='WRITE_TRUNCATE',\n",
        "        source_format = 'csv',\n",
        "        allow_quoted_newlines = 'true',\n",
        "        skip_leading_rows = 1,\n",
        "        schema_fields=[\n",
        "        {'name': 'ride_id', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "        {'name': 'rideable_type', 'type': 'INTEGER', 'mode': 'NULLABLE'},\n",
        "        {'name': 'started_at', 'type': 'DATETIME', 'mode': 'NULLABLE'},\n",
        "        {'name': 'ended_at', 'type': 'DATETIME', 'mode': 'NULLABLE'},\n",
        "        {'name': 'start_station_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
        "        {'name': 'end_station_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
        "        {'name': 'start_lat', 'type': 'FLOAT', 'mode': 'NULLABLE'},\n",
        "        {'name': 'start_lng', 'type': 'FLOAT', 'mode': 'NULLABLE'},\n",
        "        {'name': 'end_lat', 'type': 'FLOAT', 'mode': 'NULLABLE'},\n",
        "        {'name': 'end_lng', 'type': 'FLOAT', 'mode': 'NULLABLE'},\n",
        "        {'name': 'member_casual', 'type': 'INTEGER', 'mode': 'NULLABLE'},\n",
        "        {'name': 'ride_length', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
        "        {'name': 'start_date', 'type': 'DATE', 'mode': 'NULLABLE'},\n",
        "        {'name': 'x', 'type': 'FLOAT', 'mode': 'NULLABLE'},\n",
        "        {'name': 'y', 'type': 'FLOAT', 'mode': 'NULLABLE'},\n",
        "        {'name': 'distance', 'type': 'FLOAT', 'mode': 'NULLABLE'},\n",
        "        {'name': 'year', 'type': 'INTEGER', 'mode': 'NULLABLE'},\n",
        "        {'name': 'month', 'type': 'INTEGER', 'mode': 'NULLABLE'},\n",
        "        {'name': 'ride_len', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    load_dataset_ride_type = GCSToBigQueryOperator(\n",
        "        task_id = 'load_dataset_ride_type',\n",
        "        bucket = BUCKET_NAME,\n",
        "        source_objects = ['trips/Ride_Type_Data.csv'],\n",
        "        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.dataset_ride_type',\n",
        "        write_disposition='WRITE_TRUNCATE',\n",
        "        source_format = 'csv',\n",
        "        allow_quoted_newlines = 'true',\n",
        "        skip_leading_rows = 1,\n",
        "        schema_fields=[\n",
        "        {'name': 'Ride_Type', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "           {'name': 'Id', 'type': 'INTEGER', 'mode': 'REQUIRED'},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    load_dataset_station = GCSToBigQueryOperator(\n",
        "        task_id = 'load_dataset_station',\n",
        "        bucket = BUCKET_NAME,\n",
        "        source_objects = ['trips/Stations_Data.csv'],\n",
        "        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.dataset_station',\n",
        "        write_disposition='WRITE_TRUNCATE',\n",
        "        source_format = 'csv',\n",
        "        allow_quoted_newlines = 'true',\n",
        "        skip_leading_rows = 1,\n",
        "        schema_fields=[\n",
        "        {'name': 'station', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "        {'name': 'station_id', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
        "            ]\n",
        "        )\n",
        "    \n",
        "    load_dataset_rider_type = GCSToBigQueryOperator(\n",
        "        task_id = 'load_dataset_rider_type',\n",
        "        bucket = BUCKET_NAME,\n",
        "        source_objects = ['trips/Rider_Type_Data.csv'],\n",
        "        destination_project_dataset_table = f'{PROJECT_ID}:{STAGING_DATASET}.dataset_rider_type',\n",
        "        write_disposition='WRITE_TRUNCATE',\n",
        "        source_format = 'csv',\n",
        "        allow_quoted_newlines = 'true',\n",
        "        skip_leading_rows = 1,\n",
        "        schema_fields=[\n",
        "        {'name': 'Rider_Type', 'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "        {'name': 'Id', 'type': 'INTEGER', 'mode': 'NULLABLE'},\n",
        "            ]\n",
        "        )   \n",
        "    \n",
        "    check_dataset_master_trip = BigQueryCheckOperator(\n",
        "        task_id = 'check_dataset_master_trip',\n",
        "        use_legacy_sql=False,\n",
        "        location = LOCATION,\n",
        "        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.dataset_master_trip`'\n",
        "        )\n",
        "\n",
        "    check_dataset_ride_type = BigQueryCheckOperator(\n",
        "        task_id = 'check_dataset_ride_type',\n",
        "        use_legacy_sql=False,\n",
        "        location = LOCATION,\n",
        "        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.dataset_ride_type`'\n",
        "        )\n",
        "\n",
        "    check_dataset_station = BigQueryCheckOperator(\n",
        "        task_id = 'check_dataset_station',\n",
        "        use_legacy_sql=False,\n",
        "        location = LOCATION,\n",
        "        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.dataset_station`'\n",
        "        ) \n",
        "\n",
        "    check_dataset_rider_type = BigQueryCheckOperator(\n",
        "        task_id = 'check_dataset_rider_type',\n",
        "        use_legacy_sql=False,\n",
        "        location = LOCATION,\n",
        "        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{STAGING_DATASET}.dataset_rider_type`'\n",
        "        )\n",
        " \n",
        "    create_D_Table = DummyOperator(\n",
        "        task_id = 'Create_D_Table',\n",
        "        dag = dag\n",
        "        )\n",
        "\n",
        "    create_D_dataset_ayam = BigQueryOperator(\n",
        "        task_id = 'create_D_dataset_ayam',\n",
        "        use_legacy_sql = False,\n",
        "        location = LOCATION,\n",
        "        sql = './sql/D_dataset_ayam.sql'\n",
        "        )\n",
        "\n",
        "    create_D_dataset_ikan = BigQueryOperator(\n",
        "        task_id = 'create_D_dataset_ikan',\n",
        "        use_legacy_sql = False,\n",
        "        location = LOCATION,\n",
        "        sql = './sql/D_dataset_ikan.sql'\n",
        "        )   \n",
        "\n",
        "    create_D_dataset_tahu = BigQueryOperator(\n",
        "        task_id = 'create_D_dataset_tahu',\n",
        "        use_legacy_sql = False,\n",
        "        location = LOCATION,\n",
        "        sql = './sql/D_dataset_tahu.sql'\n",
        "        )\n",
        "\n",
        "    create_D_dataset_telur = BigQueryOperator(\n",
        "        task_id = 'create_D_dataset_telur',\n",
        "        use_legacy_sql = False,\n",
        "        location = LOCATION,\n",
        "        sql = './sql/D_dataset_telur.sql'\n",
        "        )\n",
        "\n",
        "    create_D_dataset_tempe = BigQueryOperator(\n",
        "        task_id = 'create_D_dataset_tempe',\n",
        "        use_legacy_sql = False,\n",
        "        location = LOCATION,\n",
        "        sql = './sql/D_dataset_tempe.sql'\n",
        "        )         \n",
        "\n",
        "    create_D_dataset_udang = BigQueryOperator(\n",
        "        task_id = 'create_D_dataset_udang',\n",
        "        use_legacy_sql = False,\n",
        "        location = LOCATION,\n",
        "        sql = './sql/D_dataset_udang.sql'\n",
        "        )\n",
        "\n",
        "    create_F_dataset_recipe = BigQueryOperator(\n",
        "        task_id = 'create_F_dataset_recipe',\n",
        "        use_legacy_sql = False,\n",
        "        location = LOCATION,\n",
        "        sql = './sql/F_dataset_recipe.sql'\n",
        "        )\n",
        "\n",
        "    check_F_dataset_recipe = BigQueryCheckOperator(\n",
        "        task_id = 'check_F_dataset_recipe',\n",
        "        use_legacy_sql=False,\n",
        "        location = LOCATION,\n",
        "        sql = f'SELECT count(*) FROM `{PROJECT_ID}.{DATASET}.F_dataset_recipe`'\n",
        "        ) \n",
        "\n",
        "    finish_pipeline = DummyOperator(\n",
        "        task_id = 'finish_pipeline',\n",
        "        dag = dag\n",
        "        ) \n",
        "start_pipeline >> load_staging_dataset\n",
        "\n",
        "load_staging_dataset >> [load_dataset_ayam, load_dataset_ikan, load_dataset_tahu, load_dataset_telur, load_dataset_tempe, load_dataset_udang]\n",
        "\n",
        "load_dataset_ayam >> check_dataset_ayam\n",
        "load_dataset_ikan >> check_dataset_ikan\n",
        "load_dataset_tahu >> check_dataset_tahu\n",
        "load_dataset_telur >> check_dataset_telur\n",
        "load_dataset_tempe >> check_dataset_tempe\n",
        "load_dataset_udang >> check_dataset_udang\n",
        "\n",
        "[check_dataset_ayam, check_dataset_ikan, check_dataset_tahu, check_dataset_telur, check_dataset_tempe, check_dataset_udang] >> create_D_Table\n",
        "\n",
        "create_D_Table >> [create_D_dataset_ayam, create_D_dataset_ikan, create_D_dataset_tahu, create_D_dataset_telur, create_D_dataset_tempe, create_D_dataset_udang]\n",
        "\n",
        "[create_D_dataset_ayam, create_D_dataset_ikan, create_D_dataset_tahu, create_D_dataset_telur, create_D_dataset_tempe, create_D_dataset_udang] >> create_F_dataset_recipe\n",
        "\n",
        "create_F_dataset_recipe >> check_F_dataset_recipe >> finish_pipeline"
      ]
    }
  ]
}